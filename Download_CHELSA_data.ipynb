{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHELSA Monthly Climate Data Downloader\n",
    "\n",
    "This script automates the download of monthly climate data from the CHELSA dataset. It reads a list of URLs from the \"envidatS3paths.txt\" file and organizes the downloaded data into folders based on specific keywords. This helps to efficiently manage and store the downloaded files.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Python Dependencies**: Ensure you have the required Python dependencies installed. You can install them using the following command:\n",
    "\n",
    "   ```bash\n",
    "   pip install requests\n",
    "   ```\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. **URL List**: Prepare a text file named \"envidatS3paths.txt\" containing the URLs of the data files you want to download. Each URL should be on a separate line.\n",
    "\n",
    "2. **Keywords**: Update the `keywords` list with the keywords that correspond to the data types you want to organize and download (e.g., \"pr\", \"rsds\", \"tasmax\", \"tasmin\", \"vpd\").\n",
    "\n",
    "3. **Open the Jupyter Notebook**: Launch your Jupyter Notebook environment.\n",
    "\n",
    "4. **Run the Notebook Cells**: Execute the notebook cells sequentially by clicking on each cell and pressing Shift + Enter. Make sure to run the cells in the correct order.\n",
    "\n",
    "5. **Output**: The script will create subfolders for each keyword (e.g., \"pr\", \"rsds\") and download the respective files into those folders. If any downloaded files are incomplete or corrupted, the script will detect and handle them by replacing or deleting the problematic files. The script will also create an \"incomplete_files.txt\" file to keep track of incomplete or corrupted downloads.\n",
    "\n",
    "## Notes\n",
    "\n",
    "- The script uses the `requests` library to handle HTTP requests and download files. Make sure you have a stable internet connection while running the script.\n",
    "- This script is tailored for the specific requirements of the CHELSA dataset. Adapt it accordingly if you're using it for a different dataset or scenario.\n",
    "- Ensure data integrity by verifying downloaded files after the script execution.\n",
    "\n",
    "## Author\n",
    "\n",
    "Script written by Luca Ferrari\n",
    "\n",
    "Contact: luca.ferrari@usys.ethz.ch\n",
    "\n",
    "For inquiries or assistance, please contact the author.\n",
    "\n",
    "**Note:** Always ensure compliance with terms of use and copyright restrictions when downloading and using external datasets.\n",
    "\n",
    "This README content was generated with the assistance of an AI language model from OpenAI. The provided content is based on user input and has been tailored to the specific requirements of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from multiprocessing import Pool\n",
    "\n",
    "keywords = [\"pr\", \"rsds\", \"tasmax\", \"tasmin\", \"vpd\"]\n",
    "with open(\"envidatS3paths.txt\") as f:\n",
    "    urls = [url.rstrip() for url in f]\n",
    "\n",
    "def download_file(url):\n",
    "    try:\n",
    "        # Extract the filename from the URL\n",
    "        filename = os.path.basename(url)\n",
    "\n",
    "        # Determine the keyword that identifies the data\n",
    "        keyword = next((kw for kw in keywords if kw in filename), None)\n",
    "\n",
    "        # If no keyword is found, skip the file\n",
    "        if not keyword:\n",
    "            print(f\"Skipping file {filename}\")\n",
    "            return\n",
    "\n",
    "        # Determine the destination folder based on the keyword\n",
    "        folder = os.path.join(os.getcwd(), keyword)\n",
    "\n",
    "        # Create the folder if it doesn't exist\n",
    "        if not os.path.exists(folder):\n",
    "            print(f\"Creating folder {folder}\")\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        # Download the file to the destination folder\n",
    "        output_file = os.path.join(folder, filename)\n",
    "        if os.path.exists(output_file):\n",
    "            # Check if the existing file is complete and not corrupted\n",
    "            existing_size = os.path.getsize(output_file)\n",
    "            headers = requests.head(url).headers\n",
    "            downloaded_size = int(headers.get(\"Content-Length\", 0))\n",
    "            if downloaded_size != existing_size:\n",
    "                # Delete the existing file and download the new file\n",
    "                print(f\"Replacing incomplete or corrupted file {output_file}\")\n",
    "                os.remove(output_file)\n",
    "                response = requests.get(url)\n",
    "                with open(output_file, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                with open(\"incomplete_files.txt\", \"a\") as f:\n",
    "                    f.write(url + \"\\n\")\n",
    "            else:\n",
    "                print(f\"Skipping existing complete file {output_file}\")\n",
    "        else:\n",
    "            # Download the file to the destination folder\n",
    "            print(f\"Downloading file {filename}\")\n",
    "            response = requests.get(url)\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            # Check if the downloaded file is complete and not corrupted\n",
    "            downloaded_size = len(response.content)\n",
    "            headers = requests.head(url).headers\n",
    "            content_length = int(headers.get(\"Content-Length\", 0))\n",
    "            if downloaded_size != content_length:\n",
    "                # Delete the incomplete or corrupted file and add its URL to the incomplete_files.txt file\n",
    "                print(f\"Deleting incomplete or corrupted file {output_file}\")\n",
    "                os.remove(output_file)\n",
    "                with open(\"incomplete_files.txt\", \"a\") as f:\n",
    "                    f.write(url + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file {url}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with Pool(8) as p:\n",
    "        p.map(download_file, urls)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
